{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Count</th>\n",
       "      <th>Kernel</th>\n",
       "      <th>Accuracy_Test_Set</th>\n",
       "      <th>Cross_Val_Scores</th>\n",
       "      <th>Mean_Cross_Val_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[0.5, 0.7272727272727273, 0.6363636363636364, ...</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.5454545454545454, ...</td>\n",
       "      <td>0.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>[0.6666666666666666, 0.5454545454545454, 0.636...</td>\n",
       "      <td>0.642424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>[0.3333333333333333, 0.5454545454545454, 0.727...</td>\n",
       "      <td>0.521212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.5454545454545454, ...</td>\n",
       "      <td>0.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>[0.4166666666666667, 0.36363636363636365, 0.54...</td>\n",
       "      <td>0.428788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[0.5833333333333334, 0.7272727272727273, 0.727...</td>\n",
       "      <td>0.625758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.5454545454545454, ...</td>\n",
       "      <td>0.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>[0.4166666666666667, 0.8181818181818182, 0.636...</td>\n",
       "      <td>0.646970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5833333333333334, 0.5454545454545454, 0.545...</td>\n",
       "      <td>0.498485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.5454545454545454, ...</td>\n",
       "      <td>0.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>150</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[0.5833333333333334, 0.45454545454545453, 0.63...</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>175</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.6363636363636364, ...</td>\n",
       "      <td>0.518182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>175</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.5454545454545454, ...</td>\n",
       "      <td>0.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>175</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.6363636363636364, ...</td>\n",
       "      <td>0.518182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>180</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.6363636363636364, ...</td>\n",
       "      <td>0.554545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>180</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.5454545454545454, ...</td>\n",
       "      <td>0.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>180</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5833333333333334, 0.7272727272727273, 0.636...</td>\n",
       "      <td>0.589394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.4166666666666667, 0.5454545454545454, 0.545...</td>\n",
       "      <td>0.519697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.5454545454545454, ...</td>\n",
       "      <td>0.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>200</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.4166666666666667, 0.6363636363636364, 0.545...</td>\n",
       "      <td>0.537879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature_Count  Kernel  Accuracy_Test_Set  \\\n",
       "0              25  linear           0.571429   \n",
       "1              25     rbf           0.357143   \n",
       "2              25    poly           0.714286   \n",
       "3              50  linear           0.857143   \n",
       "4              50     rbf           0.357143   \n",
       "5              50    poly           0.785714   \n",
       "6             100  linear           0.500000   \n",
       "7             100     rbf           0.357143   \n",
       "8             100    poly           0.428571   \n",
       "9             150  linear           0.642857   \n",
       "10            150     rbf           0.357143   \n",
       "11            150    poly           0.571429   \n",
       "12            175  linear           0.714286   \n",
       "13            175     rbf           0.357143   \n",
       "14            175    poly           0.714286   \n",
       "15            180  linear           0.642857   \n",
       "16            180     rbf           0.357143   \n",
       "17            180    poly           0.642857   \n",
       "18            200  linear           0.642857   \n",
       "19            200     rbf           0.357143   \n",
       "20            200    poly           0.642857   \n",
       "\n",
       "                                     Cross_Val_Scores  Mean_Cross_Val_Score  \n",
       "0   [0.5, 0.7272727272727273, 0.6363636363636364, ...              0.590909  \n",
       "1   [0.5, 0.5454545454545454, 0.5454545454545454, ...              0.536364  \n",
       "2   [0.6666666666666666, 0.5454545454545454, 0.636...              0.642424  \n",
       "3   [0.3333333333333333, 0.5454545454545454, 0.727...              0.521212  \n",
       "4   [0.5, 0.5454545454545454, 0.5454545454545454, ...              0.536364  \n",
       "5   [0.4166666666666667, 0.36363636363636365, 0.54...              0.428788  \n",
       "6   [0.5833333333333334, 0.7272727272727273, 0.727...              0.625758  \n",
       "7   [0.5, 0.5454545454545454, 0.5454545454545454, ...              0.536364  \n",
       "8   [0.4166666666666667, 0.8181818181818182, 0.636...              0.646970  \n",
       "9   [0.5833333333333334, 0.5454545454545454, 0.545...              0.498485  \n",
       "10  [0.5, 0.5454545454545454, 0.5454545454545454, ...              0.536364  \n",
       "11  [0.5833333333333334, 0.45454545454545453, 0.63...              0.516667  \n",
       "12  [0.5, 0.5454545454545454, 0.6363636363636364, ...              0.518182  \n",
       "13  [0.5, 0.5454545454545454, 0.5454545454545454, ...              0.536364  \n",
       "14  [0.5, 0.5454545454545454, 0.6363636363636364, ...              0.518182  \n",
       "15  [0.5, 0.5454545454545454, 0.6363636363636364, ...              0.554545  \n",
       "16  [0.5, 0.5454545454545454, 0.5454545454545454, ...              0.536364  \n",
       "17  [0.5833333333333334, 0.7272727272727273, 0.636...              0.589394  \n",
       "18  [0.4166666666666667, 0.5454545454545454, 0.545...              0.519697  \n",
       "19  [0.5, 0.5454545454545454, 0.5454545454545454, ...              0.536364  \n",
       "20  [0.4166666666666667, 0.6363636363636364, 0.545...              0.537879  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [25, 50, 100, 150,175,180, 200]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/ACER/OneDrive - University of Jaffna/UOJ/Education/Research/Data Sets/GSE140842/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # List of kernels to try\n",
    "    kernels = ['linear', 'rbf', 'poly']\n",
    "\n",
    "    for kernel in kernels:\n",
    "        # Initialize an SVM model with the current kernel\n",
    "        svm_model = SVC(kernel=kernel)\n",
    "\n",
    "        # Train the SVM model on the training data\n",
    "        svm_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the SVM model on the testing set\n",
    "        accuracy = svm_model.score(X_test, y_test)\n",
    "\n",
    "        # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "        cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5)\n",
    "\n",
    "        # Store the results in a dictionary\n",
    "        result = {\n",
    "            'Feature_Count': count,\n",
    "            'Kernel': kernel,\n",
    "            'Accuracy_Test_Set': accuracy,\n",
    "            'Cross_Val_Scores': cv_scores,\n",
    "            'Mean_Cross_Val_Score': np.mean(cv_scores)\n",
    "        }\n",
    "\n",
    "        # Append the result to the list of results\n",
    "        results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the final accuracy result table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Count</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy_Test_Set</th>\n",
       "      <th>Cross_Val_Scores</th>\n",
       "      <th>Mean_Cross_Val_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[0.8333333333333334, 0.9090909090909091, 0.545...</td>\n",
       "      <td>0.748485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5833333333333334, 0.6363636363636364, 0.636...</td>\n",
       "      <td>0.643939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[0.6666666666666666, 0.7272727272727273, 0.727...</td>\n",
       "      <td>0.678788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>[0.75, 0.6363636363636364, 0.7272727272727273,...</td>\n",
       "      <td>0.713636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[0.75, 0.8181818181818182, 0.6363636363636364,...</td>\n",
       "      <td>0.677273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5833333333333334, 0.6363636363636364, 0.636...</td>\n",
       "      <td>0.625758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[0.6666666666666666, 0.8181818181818182, 0.454...</td>\n",
       "      <td>0.678788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_Count    Algorithm  Accuracy_Test_Set  \\\n",
       "0             25  Naive Bayes           0.571429   \n",
       "1             50  Naive Bayes           0.642857   \n",
       "2            100  Naive Bayes           0.500000   \n",
       "3            150  Naive Bayes           0.714286   \n",
       "4            175  Naive Bayes           0.500000   \n",
       "5            180  Naive Bayes           0.642857   \n",
       "6            200  Naive Bayes           0.571429   \n",
       "\n",
       "                                    Cross_Val_Scores  Mean_Cross_Val_Score  \n",
       "0  [0.8333333333333334, 0.9090909090909091, 0.545...              0.748485  \n",
       "1  [0.5833333333333334, 0.6363636363636364, 0.636...              0.643939  \n",
       "2  [0.6666666666666666, 0.7272727272727273, 0.727...              0.678788  \n",
       "3  [0.75, 0.6363636363636364, 0.7272727272727273,...              0.713636  \n",
       "4  [0.75, 0.8181818181818182, 0.6363636363636364,...              0.677273  \n",
       "5  [0.5833333333333334, 0.6363636363636364, 0.636...              0.625758  \n",
       "6  [0.6666666666666666, 0.8181818181818182, 0.454...              0.678788  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB  # Import Gaussian Naive Bayes\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [25, 50, 100, 150, 175, 180, 200]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/ACER/OneDrive - University of Jaffna/UOJ/Education/Research/Data Sets/GSE140842/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Initialize a Gaussian Naive Bayes model\n",
    "    nb_model = GaussianNB()\n",
    "\n",
    "    # Train the Naive Bayes model on the training data\n",
    "    nb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the Naive Bayes model on the testing set\n",
    "    accuracy = nb_model.score(X_test, y_test)\n",
    "\n",
    "    # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "    cv_scores = cross_val_score(nb_model, X_train, y_train, cv=5)\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    result = {\n",
    "        'Feature_Count': count,\n",
    "        'Algorithm': 'Naive Bayes',  # Added to specify the algorithm used\n",
    "        'Accuracy_Test_Set': accuracy,\n",
    "        'Cross_Val_Scores': cv_scores,\n",
    "        'Mean_Cross_Val_Score': np.mean(cv_scores)\n",
    "    }\n",
    "\n",
    "    # Append the result to the list of results\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the final accuracy result table\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Count</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy_Test_Set</th>\n",
       "      <th>Cross_Val_Scores</th>\n",
       "      <th>Mean_Cross_Val_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[0.5833333333333334, 0.7272727272727273, 0.636...</td>\n",
       "      <td>0.662121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>[0.5833333333333334, 0.5454545454545454, 0.636...</td>\n",
       "      <td>0.589394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5, 0.6363636363636364, 0.6363636363636364, ...</td>\n",
       "      <td>0.518182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5, 0.7272727272727273, 0.45454545454545453,...</td>\n",
       "      <td>0.518182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.4166666666666667, 0.6363636363636364, 0.636...</td>\n",
       "      <td>0.501515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5833333333333334, 0.6363636363636364, 0.545...</td>\n",
       "      <td>0.534848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5, 0.8181818181818182, 0.5454545454545454, ...</td>\n",
       "      <td>0.554545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_Count            Algorithm  Accuracy_Test_Set  \\\n",
       "0             25  Logistic Regression           0.571429   \n",
       "1             50  Logistic Regression           0.785714   \n",
       "2            100  Logistic Regression           0.642857   \n",
       "3            150  Logistic Regression           0.642857   \n",
       "4            175  Logistic Regression           0.642857   \n",
       "5            180  Logistic Regression           0.642857   \n",
       "6            200  Logistic Regression           0.642857   \n",
       "\n",
       "                                    Cross_Val_Scores  Mean_Cross_Val_Score  \n",
       "0  [0.5833333333333334, 0.7272727272727273, 0.636...              0.662121  \n",
       "1  [0.5833333333333334, 0.5454545454545454, 0.636...              0.589394  \n",
       "2  [0.5, 0.6363636363636364, 0.6363636363636364, ...              0.518182  \n",
       "3  [0.5, 0.7272727272727273, 0.45454545454545453,...              0.518182  \n",
       "4  [0.4166666666666667, 0.6363636363636364, 0.636...              0.501515  \n",
       "5  [0.5833333333333334, 0.6363636363636364, 0.545...              0.534848  \n",
       "6  [0.5, 0.8181818181818182, 0.5454545454545454, ...              0.554545  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [25, 50, 100, 150, 175, 180, 200]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/ACER/OneDrive - University of Jaffna/UOJ/Education/Research/Data Sets/GSE140842/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Initialize a Logistic Regression model\n",
    "    # Increase max_iter to 1000 or a higher value\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # Train the Logistic Regression model on the training data\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the Logistic Regression model on the testing set\n",
    "    accuracy = lr_model.score(X_test, y_test)\n",
    "\n",
    "    # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "    cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5)\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    result = {\n",
    "        'Feature_Count': count,\n",
    "        'Algorithm': 'Logistic Regression',  # Specify the algorithm used\n",
    "        'Accuracy_Test_Set': accuracy,\n",
    "        'Cross_Val_Scores': cv_scores,\n",
    "        'Mean_Cross_Val_Score': np.mean(cv_scores)\n",
    "    }\n",
    "\n",
    "    # Append the result to the list of results\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the final accuracy result table\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Count</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy_Test_Set</th>\n",
       "      <th>Cross_Val_Scores</th>\n",
       "      <th>Mean_Cross_Val_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.6363636363636364, ...</td>\n",
       "      <td>0.572727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[0.3333333333333333, 0.5454545454545454, 0.636...</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.7272727272727273, ...</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5, 0.5454545454545454, 0.7272727272727273, ...</td>\n",
       "      <td>0.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>[0.5, 0.45454545454545453, 0.45454545454545453...</td>\n",
       "      <td>0.445455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>[0.5, 0.8181818181818182, 0.7272727272727273, ...</td>\n",
       "      <td>0.572727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[0.6666666666666666, 0.6363636363636364, 0.727...</td>\n",
       "      <td>0.587879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_Count Algorithm  Accuracy_Test_Set  \\\n",
       "0             25       ANN           0.642857   \n",
       "1             50       ANN           0.571429   \n",
       "2            100       ANN           0.642857   \n",
       "3            150       ANN           0.642857   \n",
       "4            175       ANN           0.714286   \n",
       "5            180       ANN           0.642857   \n",
       "6            200       ANN           0.571429   \n",
       "\n",
       "                                    Cross_Val_Scores  Mean_Cross_Val_Score  \n",
       "0  [0.5, 0.5454545454545454, 0.6363636363636364, ...              0.572727  \n",
       "1  [0.3333333333333333, 0.5454545454545454, 0.636...              0.466667  \n",
       "2  [0.5, 0.5454545454545454, 0.7272727272727273, ...              0.590909  \n",
       "3  [0.5, 0.5454545454545454, 0.7272727272727273, ...              0.536364  \n",
       "4  [0.5, 0.45454545454545453, 0.45454545454545453...              0.445455  \n",
       "5  [0.5, 0.8181818181818182, 0.7272727272727273, ...              0.572727  \n",
       "6  [0.6666666666666666, 0.6363636363636364, 0.727...              0.587879  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import MLPClassifier from scikit-learn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [25, 50, 100, 150, 175, 180, 200]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/ACER/OneDrive - University of Jaffna/UOJ/Education/Research/Data Sets/GSE140842/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Standardize the features (important for neural networks)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize an MLPClassifier (Artificial Neural Network)\n",
    "    ann_model = MLPClassifier(hidden_layer_sizes=(\n",
    "        100, 50), max_iter=1000, random_state=32)\n",
    "\n",
    "    # Train the ANN model on the training data\n",
    "    ann_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the ANN model on the testing set\n",
    "    accuracy = ann_model.score(X_test, y_test)\n",
    "\n",
    "    # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "    cv_scores = cross_val_score(ann_model, X_train, y_train, cv=5)\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    result = {\n",
    "        'Feature_Count': count,\n",
    "        'Algorithm': 'ANN',  # Specify the algorithm used\n",
    "        'Accuracy_Test_Set': accuracy,\n",
    "        'Cross_Val_Scores': cv_scores,\n",
    "        'Mean_Cross_Val_Score': np.mean(cv_scores)\n",
    "    }\n",
    "\n",
    "    # Append the result to the list of results\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the final accuracy result table\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Result Table for 3 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Random Forest\n",
      "Best Feature Count: 50\n",
      "Best Accuracy: 0.9285714285714286\n",
      "\n",
      "Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Accuracy on Test Set</th>\n",
       "      <th>Mean Cross-Validation Score</th>\n",
       "      <th>Score Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN</td>\n",
       "      <td>25</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>-0.141604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.161376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ANN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>-0.070872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN</td>\n",
       "      <td>150</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.463938</td>\n",
       "      <td>0.178920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ANN</td>\n",
       "      <td>175</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.426901</td>\n",
       "      <td>0.215957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ANN</td>\n",
       "      <td>180</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.479532</td>\n",
       "      <td>0.163325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANN</td>\n",
       "      <td>200</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.550682</td>\n",
       "      <td>0.092175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>25</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.608187</td>\n",
       "      <td>0.034670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>50</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.516569</td>\n",
       "      <td>0.269145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>100</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.586745</td>\n",
       "      <td>-0.158173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>150</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>-0.070175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>175</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.551657</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>180</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>0.001253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>200</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.161654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>25</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.730019</td>\n",
       "      <td>-0.158591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>50</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.606238</td>\n",
       "      <td>0.036619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>100</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.729045</td>\n",
       "      <td>-0.229045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>150</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.694932</td>\n",
       "      <td>0.019354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>175</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>-0.086466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>180</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>0.000557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>200</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.748538</td>\n",
       "      <td>-0.177109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>25</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.802144</td>\n",
       "      <td>-0.087859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>50</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.854776</td>\n",
       "      <td>0.073796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>100</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.729045</td>\n",
       "      <td>0.056669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>150</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.803119</td>\n",
       "      <td>0.054024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>175</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.765107</td>\n",
       "      <td>0.163464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>180</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.802144</td>\n",
       "      <td>-0.016430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>200</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.768031</td>\n",
       "      <td>-0.053745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.553606</td>\n",
       "      <td>-0.053606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>50</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>0.107769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SVM</td>\n",
       "      <td>100</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.536062</td>\n",
       "      <td>-0.107491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SVM</td>\n",
       "      <td>150</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.551657</td>\n",
       "      <td>-0.051657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SVM</td>\n",
       "      <td>175</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.516569</td>\n",
       "      <td>0.126288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SVM</td>\n",
       "      <td>180</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.623782</td>\n",
       "      <td>-0.052353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SVM</td>\n",
       "      <td>200</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.588694</td>\n",
       "      <td>-0.017265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Feature Count  Accuracy on Test Set  \\\n",
       "3                   ANN             25              0.428571   \n",
       "8                   ANN             50              0.642857   \n",
       "13                  ANN            100              0.571429   \n",
       "18                  ANN            150              0.642857   \n",
       "23                  ANN            175              0.642857   \n",
       "28                  ANN            180              0.642857   \n",
       "33                  ANN            200              0.642857   \n",
       "2   Logistic Regression             25              0.642857   \n",
       "7   Logistic Regression             50              0.785714   \n",
       "12  Logistic Regression            100              0.428571   \n",
       "17  Logistic Regression            150              0.500000   \n",
       "22  Logistic Regression            175              0.642857   \n",
       "27  Logistic Regression            180              0.571429   \n",
       "32  Logistic Regression            200              0.714286   \n",
       "1           Naive Bayes             25              0.571429   \n",
       "6           Naive Bayes             50              0.642857   \n",
       "11          Naive Bayes            100              0.500000   \n",
       "16          Naive Bayes            150              0.714286   \n",
       "21          Naive Bayes            175              0.571429   \n",
       "26          Naive Bayes            180              0.642857   \n",
       "31          Naive Bayes            200              0.571429   \n",
       "4         Random Forest             25              0.714286   \n",
       "9         Random Forest             50              0.928571   \n",
       "14        Random Forest            100              0.785714   \n",
       "19        Random Forest            150              0.857143   \n",
       "24        Random Forest            175              0.928571   \n",
       "29        Random Forest            180              0.785714   \n",
       "34        Random Forest            200              0.714286   \n",
       "0                   SVM             25              0.500000   \n",
       "5                   SVM             50              0.642857   \n",
       "10                  SVM            100              0.428571   \n",
       "15                  SVM            150              0.500000   \n",
       "20                  SVM            175              0.642857   \n",
       "25                  SVM            180              0.571429   \n",
       "30                  SVM            200              0.571429   \n",
       "\n",
       "    Mean Cross-Validation Score  Score Difference  \n",
       "3                      0.570175         -0.141604  \n",
       "8                      0.481481          0.161376  \n",
       "13                     0.642300         -0.070872  \n",
       "18                     0.463938          0.178920  \n",
       "23                     0.426901          0.215957  \n",
       "28                     0.479532          0.163325  \n",
       "33                     0.550682          0.092175  \n",
       "2                      0.608187          0.034670  \n",
       "7                      0.516569          0.269145  \n",
       "12                     0.586745         -0.158173  \n",
       "17                     0.570175         -0.070175  \n",
       "22                     0.551657          0.091200  \n",
       "27                     0.570175          0.001253  \n",
       "32                     0.552632          0.161654  \n",
       "1                      0.730019         -0.158591  \n",
       "6                      0.606238          0.036619  \n",
       "11                     0.729045         -0.229045  \n",
       "16                     0.694932          0.019354  \n",
       "21                     0.657895         -0.086466  \n",
       "26                     0.642300          0.000557  \n",
       "31                     0.748538         -0.177109  \n",
       "4                      0.802144         -0.087859  \n",
       "9                      0.854776          0.073796  \n",
       "14                     0.729045          0.056669  \n",
       "19                     0.803119          0.054024  \n",
       "24                     0.765107          0.163464  \n",
       "29                     0.802144         -0.016430  \n",
       "34                     0.768031         -0.053745  \n",
       "0                      0.553606         -0.053606  \n",
       "5                      0.535088          0.107769  \n",
       "10                     0.536062         -0.107491  \n",
       "15                     0.551657         -0.051657  \n",
       "20                     0.516569          0.126288  \n",
       "25                     0.623782         -0.052353  \n",
       "30                     0.588694         -0.017265  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [25, 50, 100, 150, 175, 180, 200]\n",
    "\n",
    "# List of models to compare, including Random Forest\n",
    "models = {\n",
    "    'SVM': SVC(kernel='linear'),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'ANN': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000),\n",
    "    # Add Random Forest\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Variables to track the best accuracy and model\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_feature_count = None\n",
    "\n",
    "# Threshold for identifying potential overfitting\n",
    "overfitting_threshold = 0.05  # You can adjust this threshold as needed\n",
    "\n",
    "# Lists to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/ACER/OneDrive - University of Jaffna/UOJ/Education/Research/Data Sets/GSE140842/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Standardize the features (important for some models)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model on the testing set\n",
    "        accuracy = model.score(X_test, y_test)\n",
    "\n",
    "        # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=3)\n",
    "\n",
    "        # Calculate the mean cross-validation score\n",
    "        mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "        # Calculate the difference between test accuracy and mean cross-validation score\n",
    "        score_difference = accuracy - mean_cv_score\n",
    "\n",
    "        # Check if the model may be overfitting based on the threshold\n",
    "        if score_difference <= 0.1:\n",
    "            # The model is not overfitting\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model_name\n",
    "                best_feature_count = count\n",
    "\n",
    "        # Append the results for the current model and feature count to the list\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Feature Count': count,\n",
    "            'Accuracy on Test Set': accuracy,\n",
    "            'Mean Cross-Validation Score': mean_cv_score,\n",
    "            'Score Difference': score_difference\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results DataFrame by model name\n",
    "results_df = results_df.sort_values(by=['Model', 'Feature Count'])\n",
    "\n",
    "# Print the best model and feature count\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Best Feature Count: {best_feature_count}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(\"\\nResults DataFrame:\")\n",
    "results_df\n",
    "\n",
    "# # Create a table plot of the results DataFrame\n",
    "# fig, ax = plt.subplots(figsize=(20, 12))\n",
    "# ax.axis('tight')\n",
    "# ax.axis('off')\n",
    "# ax.table(cellText=results_df.values, colLabels=results_df.columns,\n",
    "#          cellLoc='center', loc='center')\n",
    "\n",
    "# # Save the table plot as an image\n",
    "# table_image_path = 'results_table_fold3.png'\n",
    "# plt.savefig(table_image_path, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Print the path to the saved image\n",
    "# print(f\"Results table image saved at: {table_image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Result Table for 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 10 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 10 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 11 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 11 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 12 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 12 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 13 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 13 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 14 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 14 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 15 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 15 done for XGBoost\n",
      "Best Model: XGBoost\n",
      "Best Feature Count: 10\n",
      "Best Accuracy: 0.7142857142857143\n",
      "\n",
      "Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Best Hyperparameters</th>\n",
       "      <th>Accuracy on Test Set</th>\n",
       "      <th>Mean CV Score</th>\n",
       "      <th>Std CV Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.831089</td>\n",
       "      <td>0.024359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>11</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.863945</td>\n",
       "      <td>0.032034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>12</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.853648</td>\n",
       "      <td>0.032795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>13</td>\n",
       "      <td>{'bootstrap': True, 'max_depth': None, 'max_fe...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.032239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>14</td>\n",
       "      <td>{'bootstrap': True, 'max_depth': None, 'max_fe...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.828563</td>\n",
       "      <td>0.027094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>15</td>\n",
       "      <td>{'bootstrap': True, 'max_depth': None, 'max_fe...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.852525</td>\n",
       "      <td>0.034863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>10</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.739182</td>\n",
       "      <td>0.107768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>11</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.720970</td>\n",
       "      <td>0.099187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>12</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 100, 'min_c...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.711439</td>\n",
       "      <td>0.100313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>13</td>\n",
       "      <td>{'subsample': 1.0, 'n_estimators': 50, 'min_ch...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.715894</td>\n",
       "      <td>0.106263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>14</td>\n",
       "      <td>{'subsample': 1.0, 'n_estimators': 50, 'min_ch...</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.707500</td>\n",
       "      <td>0.102468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>15</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.669606</td>\n",
       "      <td>0.095153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Feature Count  \\\n",
       "0   Random Forest             10   \n",
       "2   Random Forest             11   \n",
       "4   Random Forest             12   \n",
       "6   Random Forest             13   \n",
       "8   Random Forest             14   \n",
       "10  Random Forest             15   \n",
       "1         XGBoost             10   \n",
       "3         XGBoost             11   \n",
       "5         XGBoost             12   \n",
       "7         XGBoost             13   \n",
       "9         XGBoost             14   \n",
       "11        XGBoost             15   \n",
       "\n",
       "                                 Best Hyperparameters  Accuracy on Test Set  \\\n",
       "0   {'bootstrap': False, 'max_depth': None, 'max_f...              0.571429   \n",
       "2   {'bootstrap': False, 'max_depth': None, 'max_f...              0.571429   \n",
       "4   {'bootstrap': False, 'max_depth': None, 'max_f...              0.500000   \n",
       "6   {'bootstrap': True, 'max_depth': None, 'max_fe...              0.571429   \n",
       "8   {'bootstrap': True, 'max_depth': None, 'max_fe...              0.714286   \n",
       "10  {'bootstrap': True, 'max_depth': None, 'max_fe...              0.714286   \n",
       "1   {'subsample': 0.7, 'n_estimators': 200, 'min_c...              0.714286   \n",
       "3   {'subsample': 0.7, 'n_estimators': 200, 'min_c...              0.642857   \n",
       "5   {'subsample': 0.7, 'n_estimators': 100, 'min_c...              0.571429   \n",
       "7   {'subsample': 1.0, 'n_estimators': 50, 'min_ch...              0.571429   \n",
       "9   {'subsample': 1.0, 'n_estimators': 50, 'min_ch...              0.642857   \n",
       "11  {'subsample': 0.7, 'n_estimators': 200, 'min_c...              0.571429   \n",
       "\n",
       "    Mean CV Score  Std CV Score  \n",
       "0        0.831089      0.024359  \n",
       "2        0.863945      0.032034  \n",
       "4        0.853648      0.032795  \n",
       "6        0.806061      0.032239  \n",
       "8        0.828563      0.027094  \n",
       "10       0.852525      0.034863  \n",
       "1        0.739182      0.107768  \n",
       "3        0.720970      0.099187  \n",
       "5        0.711439      0.100313  \n",
       "7        0.715894      0.106263  \n",
       "9        0.707500      0.102468  \n",
       "11       0.669606      0.095153  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [10,11,12,13,14,15]\n",
    "\n",
    "# List of models to compare, including Random Forest\n",
    "models = {\n",
    "    # 'SVM': SVC(kernel='linear'),\n",
    "    # 'Naive Bayes': GaussianNB(),\n",
    "    # 'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    # 'ANN': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "}\n",
    "# xgbOOST\n",
    "# Variables to track the best accuracy and model\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_feature_count = None\n",
    "\n",
    "# Threshold for identifying potential overfitting\n",
    "overfitting_threshold = 0.05  \n",
    "\n",
    "# Lists to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/ACER/OneDrive - University of Jaffna/UOJ/Education/Research/Data Sets/GSE140842/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Standardize the features (important for some models)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Define hyperparameter grids for GridSearchCV\n",
    "        param_grid_rf = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "\n",
    "        param_grid_xgb = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "            'max_depth': [3, 4, 5, 6],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'gamma': [0, 0.1, 0.2, 0.3]\n",
    "        }\n",
    "\n",
    "        if model_name == 'Random Forest':\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid_rf,\n",
    "                                       scoring='accuracy', cv=5, n_jobs=-1, verbose=2)\n",
    "        elif model_name == 'XGBoost':\n",
    "            grid_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid_xgb,\n",
    "                                             n_iter=100, scoring='accuracy', cv=5, n_jobs=-1, random_state=42, verbose=2)\n",
    "\n",
    "        # Fit the grid search to your data\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best hyperparameters and model\n",
    "        best_params = grid_search.best_params_\n",
    "        best_accuracy_cv = grid_search.best_score_\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Get the cross-validation scores\n",
    "        cross_val_scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "        # Store mean and standard deviation of cross-validation scores\n",
    "        mean_cv_score = np.mean(cross_val_scores)\n",
    "        std_cv_score = np.std(cross_val_scores)\n",
    "\n",
    "        # Evaluate the best model on the testing set\n",
    "        accuracy_test = best_model.score(X_test, y_test)\n",
    "\n",
    "        # Check if the model may be overfitting based on the threshold\n",
    "        if (accuracy_test - best_accuracy_cv) <= overfitting_threshold:\n",
    "            if accuracy_test > best_accuracy:\n",
    "                best_accuracy = accuracy_test\n",
    "                best_model_name = model_name\n",
    "                best_feature_count = count\n",
    "\n",
    "        # Append the results for the current model and feature count to the list\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Feature Count': count,\n",
    "            'Best Hyperparameters': best_params,\n",
    "            'Accuracy on Test Set': accuracy_test,\n",
    "            'Mean CV Score': mean_cv_score,\n",
    "            'Std CV Score': std_cv_score\n",
    "        })\n",
    "\n",
    "        # Print feature count done\n",
    "        print(f\"Feature count {count} done for {model_name}\")\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results DataFrame by model name\n",
    "results_df = results_df.sort_values(by=['Model', 'Feature Count'])\n",
    "\n",
    "# Print the best model and feature count\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Feature Count: {best_feature_count}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(\"\\nResults DataFrame:\")\n",
    "results_df\n",
    "\n",
    "# # Create a table plot of the results DataFrame\n",
    "# fig, ax = plt.subplots(figsize=(20, 12))\n",
    "# ax.axis('tight')\n",
    "# ax.axis('off')\n",
    "# ax.table(cellText=results_df.values, colLabels=results_df.columns,\n",
    "#          cellLoc='center', loc='center')\n",
    "\n",
    "# # Save the table plot as an image\n",
    "# table_image_path = 'results_table_fold5.png'\n",
    "# plt.savefig(table_image_path, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Print the path to the saved image\n",
    "# print(f\"Results table image saved at: {table_image_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
