{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Count</th>\n",
       "      <th>Kernel</th>\n",
       "      <th>Accuracy_Test_Set</th>\n",
       "      <th>Cross_Val_Scores</th>\n",
       "      <th>Mean_Cross_Val_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>[0.5384615384615384, 0.5384615384615384, 0.538...</td>\n",
       "      <td>0.556410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6153846153846154, 0.6153846153846154, 0.384...</td>\n",
       "      <td>0.556410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.7692307692307693, 0.5384615384615384, 0.307...</td>\n",
       "      <td>0.506410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>[0.6153846153846154, 0.6153846153846154, 0.384...</td>\n",
       "      <td>0.523077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.5384615384615384, 0.6153846153846154, 0.384...</td>\n",
       "      <td>0.541026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.7692307692307693, 0.5384615384615384, 0.307...</td>\n",
       "      <td>0.506410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6153846153846154, 0.8461538461538461, 0.538...</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.6153846153846154, 0.8461538461538461, 0.384...</td>\n",
       "      <td>0.652564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.7692307692307693, 0.6153846153846154, 0.384...</td>\n",
       "      <td>0.603846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>[0.6153846153846154, 0.6923076923076923, 0.692...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.6153846153846154, 0.7692307692307693, 0.384...</td>\n",
       "      <td>0.637179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.7692307692307693, 0.6153846153846154, 0.384...</td>\n",
       "      <td>0.587179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.923...</td>\n",
       "      <td>0.775641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6923076923076923, 0.7692307692307693, 0.384...</td>\n",
       "      <td>0.635897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.384...</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.923...</td>\n",
       "      <td>0.775641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>150</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6923076923076923, 0.7692307692307693, 0.384...</td>\n",
       "      <td>0.635897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>150</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.384...</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>175</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.923...</td>\n",
       "      <td>0.775641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>175</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6923076923076923, 0.7692307692307693, 0.384...</td>\n",
       "      <td>0.635897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>175</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.384...</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>180</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.923...</td>\n",
       "      <td>0.758974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>180</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6923076923076923, 0.7692307692307693, 0.384...</td>\n",
       "      <td>0.635897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>180</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.384...</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>200</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.923...</td>\n",
       "      <td>0.775641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>200</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6153846153846154, 0.6153846153846154, 0.461...</td>\n",
       "      <td>0.638462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>200</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.6923076923076923, 0.46153846153846156, 0.38...</td>\n",
       "      <td>0.507692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature_Count  Kernel  Accuracy_Test_Set  \\\n",
       "0              10  linear             0.4375   \n",
       "1              10     rbf             0.6250   \n",
       "2              10    poly             0.6250   \n",
       "3              15  linear             0.4375   \n",
       "4              15     rbf             0.6250   \n",
       "5              15    poly             0.5000   \n",
       "6              25  linear             0.6250   \n",
       "7              25     rbf             0.6875   \n",
       "8              25    poly             0.5000   \n",
       "9              50  linear             0.4375   \n",
       "10             50     rbf             0.6875   \n",
       "11             50    poly             0.5000   \n",
       "12            100  linear             0.6875   \n",
       "13            100     rbf             0.6250   \n",
       "14            100    poly             0.5625   \n",
       "15            150  linear             0.7500   \n",
       "16            150     rbf             0.6250   \n",
       "17            150    poly             0.5625   \n",
       "18            175  linear             0.6875   \n",
       "19            175     rbf             0.6250   \n",
       "20            175    poly             0.5625   \n",
       "21            180  linear             0.6875   \n",
       "22            180     rbf             0.6250   \n",
       "23            180    poly             0.5625   \n",
       "24            200  linear             0.5625   \n",
       "25            200     rbf             0.6250   \n",
       "26            200    poly             0.5625   \n",
       "\n",
       "                                     Cross_Val_Scores  Mean_Cross_Val_Score  \n",
       "0   [0.5384615384615384, 0.5384615384615384, 0.538...              0.556410  \n",
       "1   [0.6153846153846154, 0.6153846153846154, 0.384...              0.556410  \n",
       "2   [0.7692307692307693, 0.5384615384615384, 0.307...              0.506410  \n",
       "3   [0.6153846153846154, 0.6153846153846154, 0.384...              0.523077  \n",
       "4   [0.5384615384615384, 0.6153846153846154, 0.384...              0.541026  \n",
       "5   [0.7692307692307693, 0.5384615384615384, 0.307...              0.506410  \n",
       "6   [0.6153846153846154, 0.8461538461538461, 0.538...              0.716667  \n",
       "7   [0.6153846153846154, 0.8461538461538461, 0.384...              0.652564  \n",
       "8   [0.7692307692307693, 0.6153846153846154, 0.384...              0.603846  \n",
       "9   [0.6153846153846154, 0.6923076923076923, 0.692...              0.700000  \n",
       "10  [0.6153846153846154, 0.7692307692307693, 0.384...              0.637179  \n",
       "11  [0.7692307692307693, 0.6153846153846154, 0.384...              0.587179  \n",
       "12  [0.7692307692307693, 0.7692307692307693, 0.923...              0.775641  \n",
       "13  [0.6923076923076923, 0.7692307692307693, 0.384...              0.635897  \n",
       "14  [0.7692307692307693, 0.7692307692307693, 0.384...              0.634615  \n",
       "15  [0.7692307692307693, 0.7692307692307693, 0.923...              0.775641  \n",
       "16  [0.6923076923076923, 0.7692307692307693, 0.384...              0.635897  \n",
       "17  [0.7692307692307693, 0.7692307692307693, 0.384...              0.634615  \n",
       "18  [0.7692307692307693, 0.7692307692307693, 0.923...              0.775641  \n",
       "19  [0.6923076923076923, 0.7692307692307693, 0.384...              0.635897  \n",
       "20  [0.7692307692307693, 0.7692307692307693, 0.384...              0.634615  \n",
       "21  [0.7692307692307693, 0.7692307692307693, 0.923...              0.758974  \n",
       "22  [0.6923076923076923, 0.7692307692307693, 0.384...              0.635897  \n",
       "23  [0.7692307692307693, 0.7692307692307693, 0.384...              0.634615  \n",
       "24  [0.7692307692307693, 0.7692307692307693, 0.923...              0.775641  \n",
       "25  [0.6153846153846154, 0.6153846153846154, 0.461...              0.638462  \n",
       "26  [0.6923076923076923, 0.46153846153846156, 0.38...              0.507692  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [10, 15, 25, 50, 100, 150, 175, 180, 200]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/Ashfa Fathima/OneDrive - University of Jaffna/Research/Data Sets/GSE160310/smallRNA/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Disese Group', axis=1)\n",
    "    y = df['Disese Group']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # List of kernels to try\n",
    "    kernels = ['linear', 'rbf', 'poly']\n",
    "\n",
    "    for kernel in kernels:\n",
    "        # Initialize an SVM model with the current kernel\n",
    "        svm_model = SVC(kernel=kernel)\n",
    "\n",
    "        # Train the SVM model on the training data\n",
    "        svm_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the SVM model on the testing set\n",
    "        accuracy = svm_model.score(X_test, y_test)\n",
    "\n",
    "        # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "        cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5)\n",
    "\n",
    "        # Store the results in a dictionary\n",
    "        result = {\n",
    "            'Feature_Count': count,\n",
    "            'Kernel': kernel,\n",
    "            'Accuracy_Test_Set': accuracy,\n",
    "            'Cross_Val_Scores': cv_scores,\n",
    "            'Mean_Cross_Val_Score': np.mean(cv_scores)\n",
    "        }\n",
    "\n",
    "        # Append the result to the list of results\n",
    "        results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the final accuracy result table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Count</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy_Test_Set</th>\n",
       "      <th>Cross_Val_Scores</th>\n",
       "      <th>Mean_Cross_Val_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>[0.6153846153846154, 0.6923076923076923, 0.307...</td>\n",
       "      <td>0.539744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.6923076923076923, 0.7692307692307693, 0.307...</td>\n",
       "      <td>0.570513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>[0.46153846153846156, 0.7692307692307693, 0.30...</td>\n",
       "      <td>0.591026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>[0.38461538461538464, 0.7692307692307693, 0.30...</td>\n",
       "      <td>0.542308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.5384615384615384, 0.6153846153846154, 0.307...</td>\n",
       "      <td>0.525641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>150</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.46153846153846156, 0.6923076923076923, 0.23...</td>\n",
       "      <td>0.510256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.5384615384615384, 0.6923076923076923, 0.230...</td>\n",
       "      <td>0.508974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>180</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>[0.5384615384615384, 0.6923076923076923, 0.307...</td>\n",
       "      <td>0.541026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>[0.46153846153846156, 0.6923076923076923, 0.23...</td>\n",
       "      <td>0.493590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_Count    Algorithm  Accuracy_Test_Set  \\\n",
       "0             10  Naive Bayes             0.7500   \n",
       "1             15  Naive Bayes             0.6875   \n",
       "2             25  Naive Bayes             0.8125   \n",
       "3             50  Naive Bayes             0.7500   \n",
       "4            100  Naive Bayes             0.6250   \n",
       "5            150  Naive Bayes             0.6250   \n",
       "6            175  Naive Bayes             0.6875   \n",
       "7            180  Naive Bayes             0.7500   \n",
       "8            200  Naive Bayes             0.7500   \n",
       "\n",
       "                                    Cross_Val_Scores  Mean_Cross_Val_Score  \n",
       "0  [0.6153846153846154, 0.6923076923076923, 0.307...              0.539744  \n",
       "1  [0.6923076923076923, 0.7692307692307693, 0.307...              0.570513  \n",
       "2  [0.46153846153846156, 0.7692307692307693, 0.30...              0.591026  \n",
       "3  [0.38461538461538464, 0.7692307692307693, 0.30...              0.542308  \n",
       "4  [0.5384615384615384, 0.6153846153846154, 0.307...              0.525641  \n",
       "5  [0.46153846153846156, 0.6923076923076923, 0.23...              0.510256  \n",
       "6  [0.5384615384615384, 0.6923076923076923, 0.230...              0.508974  \n",
       "7  [0.5384615384615384, 0.6923076923076923, 0.307...              0.541026  \n",
       "8  [0.46153846153846156, 0.6923076923076923, 0.23...              0.493590  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB  # Import Gaussian Naive Bayes\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [10, 15, 25, 50, 100, 150, 175, 180, 200]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/Ashfa Fathima/OneDrive - University of Jaffna/Research/Data Sets/GSE160310/smallRNA/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Disese Group', axis=1)\n",
    "    y = df['Disese Group']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Initialize a Gaussian Naive Bayes model\n",
    "    nb_model = GaussianNB()\n",
    "\n",
    "    # Train the Naive Bayes model on the training data\n",
    "    nb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the Naive Bayes model on the testing set\n",
    "    accuracy = nb_model.score(X_test, y_test)\n",
    "\n",
    "    # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "    cv_scores = cross_val_score(nb_model, X_train, y_train, cv=5)\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    result = {\n",
    "        'Feature_Count': count,\n",
    "        'Algorithm': 'Naive Bayes',  # Added to specify the algorithm used\n",
    "        'Accuracy_Test_Set': accuracy,\n",
    "        'Cross_Val_Scores': cv_scores,\n",
    "        'Mean_Cross_Val_Score': np.mean(cv_scores)\n",
    "    }\n",
    "\n",
    "    # Append the result to the list of results\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the final accuracy result table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Ashfa Fathima\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Count</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy_Test_Set</th>\n",
       "      <th>Cross_Val_Scores</th>\n",
       "      <th>Mean_Cross_Val_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6153846153846154, 0.6923076923076923, 0.615...</td>\n",
       "      <td>0.617949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.6153846153846154, 0.46153846153846156, 0.30...</td>\n",
       "      <td>0.476923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>[0.46153846153846156, 0.6153846153846154, 0.76...</td>\n",
       "      <td>0.719231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.6153846153846154, 0.6153846153846154, 0.846...</td>\n",
       "      <td>0.765385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.5384615384615384, 0.5384615384615384, 0.923...</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>150</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6923076923076923, 0.6153846153846154, 0.923...</td>\n",
       "      <td>0.746154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.6923076923076923, 0.6153846153846154, 0.923...</td>\n",
       "      <td>0.762821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>180</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6923076923076923, 0.6153846153846154, 0.923...</td>\n",
       "      <td>0.762821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6153846153846154, 0.6923076923076923, 0.923...</td>\n",
       "      <td>0.729487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_Count            Algorithm  Accuracy_Test_Set  \\\n",
       "0             10  Logistic Regression             0.6250   \n",
       "1             15  Logistic Regression             0.5625   \n",
       "2             25  Logistic Regression             0.4375   \n",
       "3             50  Logistic Regression             0.5000   \n",
       "4            100  Logistic Regression             0.6875   \n",
       "5            150  Logistic Regression             0.6250   \n",
       "6            175  Logistic Regression             0.6875   \n",
       "7            180  Logistic Regression             0.6250   \n",
       "8            200  Logistic Regression             0.6250   \n",
       "\n",
       "                                    Cross_Val_Scores  Mean_Cross_Val_Score  \n",
       "0  [0.6153846153846154, 0.6923076923076923, 0.615...              0.617949  \n",
       "1  [0.6153846153846154, 0.46153846153846156, 0.30...              0.476923  \n",
       "2  [0.46153846153846156, 0.6153846153846154, 0.76...              0.719231  \n",
       "3  [0.6153846153846154, 0.6153846153846154, 0.846...              0.765385  \n",
       "4  [0.5384615384615384, 0.5384615384615384, 0.923...              0.716667  \n",
       "5  [0.6923076923076923, 0.6153846153846154, 0.923...              0.746154  \n",
       "6  [0.6923076923076923, 0.6153846153846154, 0.923...              0.762821  \n",
       "7  [0.6923076923076923, 0.6153846153846154, 0.923...              0.762821  \n",
       "8  [0.6153846153846154, 0.6923076923076923, 0.923...              0.729487  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [10, 15, 25, 50, 100, 150, 175, 180, 200]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/Ashfa Fathima/OneDrive - University of Jaffna/Research/Data Sets/GSE160310/smallRNA/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Disese Group', axis=1)\n",
    "    y = df['Disese Group']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Initialize a Logistic Regression model\n",
    "    # Increase max_iter to 1000 or a higher value\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # Train the Logistic Regression model on the training data\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the Logistic Regression model on the testing set\n",
    "    accuracy = lr_model.score(X_test, y_test)\n",
    "\n",
    "    # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "    cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5)\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    result = {\n",
    "        'Feature_Count': count,\n",
    "        'Algorithm': 'Logistic Regression',  # Specify the algorithm used\n",
    "        'Accuracy_Test_Set': accuracy,\n",
    "        'Cross_Val_Scores': cv_scores,\n",
    "        'Mean_Cross_Val_Score': np.mean(cv_scores)\n",
    "    }\n",
    "\n",
    "    # Append the result to the list of results\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the final accuracy result table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Count</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy_Test_Set</th>\n",
       "      <th>Cross_Val_Scores</th>\n",
       "      <th>Mean_Cross_Val_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.5384615384615384, 0.7692307692307693, 0.615...</td>\n",
       "      <td>0.701282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.6923076923076923, 0.6923076923076923, 0.615...</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>[0.9230769230769231, 0.8461538461538461, 0.692...</td>\n",
       "      <td>0.842308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>[0.6153846153846154, 0.6923076923076923, 0.846...</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.7692307692307693, 0.7692307692307693, 0.615...</td>\n",
       "      <td>0.697436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>150</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>[0.6923076923076923, 0.7692307692307693, 0.615...</td>\n",
       "      <td>0.682051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>[0.6153846153846154, 0.7692307692307693, 0.615...</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>180</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>[0.6923076923076923, 0.8461538461538461, 0.692...</td>\n",
       "      <td>0.712821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200</td>\n",
       "      <td>ANN</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>[0.6153846153846154, 0.8461538461538461, 0.615...</td>\n",
       "      <td>0.715385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_Count Algorithm  Accuracy_Test_Set  \\\n",
       "0             10       ANN             0.6875   \n",
       "1             15       ANN             0.5625   \n",
       "2             25       ANN             0.8125   \n",
       "3             50       ANN             0.5625   \n",
       "4            100       ANN             0.6250   \n",
       "5            150       ANN             0.6250   \n",
       "6            175       ANN             0.8125   \n",
       "7            180       ANN             0.8125   \n",
       "8            200       ANN             0.6875   \n",
       "\n",
       "                                    Cross_Val_Scores  Mean_Cross_Val_Score  \n",
       "0  [0.5384615384615384, 0.7692307692307693, 0.615...              0.701282  \n",
       "1  [0.6923076923076923, 0.6923076923076923, 0.615...              0.650000  \n",
       "2  [0.9230769230769231, 0.8461538461538461, 0.692...              0.842308  \n",
       "3  [0.6153846153846154, 0.6923076923076923, 0.846...              0.730769  \n",
       "4  [0.7692307692307693, 0.7692307692307693, 0.615...              0.697436  \n",
       "5  [0.6923076923076923, 0.7692307692307693, 0.615...              0.682051  \n",
       "6  [0.6153846153846154, 0.7692307692307693, 0.615...              0.683333  \n",
       "7  [0.6923076923076923, 0.8461538461538461, 0.692...              0.712821  \n",
       "8  [0.6153846153846154, 0.8461538461538461, 0.615...              0.715385  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import MLPClassifier from scikit-learn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [10, 15, 25, 50, 100, 150, 175, 180, 200]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/Ashfa Fathima/OneDrive - University of Jaffna/Research/Data Sets/GSE160310/smallRNA/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Disese Group', axis=1)\n",
    "    y = df['Disese Group']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Standardize the features (important for neural networks)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize an MLPClassifier (Artificial Neural Network)\n",
    "    ann_model = MLPClassifier(hidden_layer_sizes=(\n",
    "        100, 50), max_iter=1000, random_state=32)\n",
    "\n",
    "    # Train the ANN model on the training data\n",
    "    ann_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the ANN model on the testing set\n",
    "    accuracy = ann_model.score(X_test, y_test)\n",
    "\n",
    "    # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "    cv_scores = cross_val_score(ann_model, X_train, y_train, cv=5)\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    result = {\n",
    "        'Feature_Count': count,\n",
    "        'Algorithm': 'ANN',  # Specify the algorithm used\n",
    "        'Accuracy_Test_Set': accuracy,\n",
    "        'Cross_Val_Scores': cv_scores,\n",
    "        'Mean_Cross_Val_Score': np.mean(cv_scores)\n",
    "    }\n",
    "\n",
    "    # Append the result to the list of results\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the final accuracy result table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Result Table for 3 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Random Forest\n",
      "Best Feature Count: 100\n",
      "Best Accuracy: 0.8125\n",
      "\n",
      "Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Accuracy on Test Set</th>\n",
       "      <th>Mean Cross-Validation Score</th>\n",
       "      <th>Score Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANN</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.587302</td>\n",
       "      <td>-0.024802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ANN</td>\n",
       "      <td>25</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.067460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ANN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>-0.276786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ANN</td>\n",
       "      <td>150</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>-0.010913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ANN</td>\n",
       "      <td>175</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ANN</td>\n",
       "      <td>180</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>-0.042659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ANN</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.161706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.067460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>15</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.129960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>25</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>-0.043651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>50</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>-0.089286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>-0.370040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>-0.011905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>175</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>-0.026786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>180</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.144841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>200</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-0.152778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>-0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>15</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.272817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>25</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>-0.023810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.008929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.587302</td>\n",
       "      <td>-0.212302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>150</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.101190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>175</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.492063</td>\n",
       "      <td>0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>180</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.053571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>200</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>-0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>15</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>-0.011905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>25</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>50</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.019841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.066468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>150</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.113095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>175</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.082341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>180</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.128968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.128968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.634921</td>\n",
       "      <td>0.115079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>15</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.099206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SVM</td>\n",
       "      <td>25</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>-0.059524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SVM</td>\n",
       "      <td>50</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>-0.136905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SVM</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SVM</td>\n",
       "      <td>150</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.082341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SVM</td>\n",
       "      <td>175</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>-0.105159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SVM</td>\n",
       "      <td>180</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SVM</td>\n",
       "      <td>200</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>-0.262897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Feature Count  Accuracy on Test Set  \\\n",
       "3                   ANN             10                0.7500   \n",
       "8                   ANN             15                0.5625   \n",
       "13                  ANN             25                0.7500   \n",
       "18                  ANN             50                0.7500   \n",
       "23                  ANN            100                0.4375   \n",
       "28                  ANN            150                0.6875   \n",
       "33                  ANN            175                0.6875   \n",
       "38                  ANN            180                0.6875   \n",
       "43                  ANN            200                0.8125   \n",
       "2   Logistic Regression             10                0.7500   \n",
       "7   Logistic Regression             15                0.8125   \n",
       "12  Logistic Regression             25                0.7500   \n",
       "17  Logistic Regression             50                0.6250   \n",
       "22  Logistic Regression            100                0.3125   \n",
       "27  Logistic Regression            150                0.7500   \n",
       "32  Logistic Regression            175                0.6875   \n",
       "37  Logistic Regression            180                0.8750   \n",
       "42  Logistic Regression            200                0.6250   \n",
       "1           Naive Bayes             10                0.5000   \n",
       "6           Naive Bayes             15                0.8125   \n",
       "11          Naive Bayes             25                0.5000   \n",
       "16          Naive Bayes             50                0.5625   \n",
       "21          Naive Bayes            100                0.3750   \n",
       "26          Naive Bayes            150                0.6250   \n",
       "31          Naive Bayes            175                0.5000   \n",
       "36          Naive Bayes            180                0.6250   \n",
       "41          Naive Bayes            200                0.5000   \n",
       "4         Random Forest             10                0.7500   \n",
       "9         Random Forest             15                0.7500   \n",
       "14        Random Forest             25                0.7500   \n",
       "19        Random Forest             50                0.7500   \n",
       "24        Random Forest            100                0.8125   \n",
       "29        Random Forest            150                0.8750   \n",
       "34        Random Forest            175                0.8125   \n",
       "39        Random Forest            180                0.8750   \n",
       "44        Random Forest            200                0.8750   \n",
       "0                   SVM             10                0.7500   \n",
       "5                   SVM             15                0.7500   \n",
       "10                  SVM             25                0.7500   \n",
       "15                  SVM             50                0.6250   \n",
       "20                  SVM            100                0.3125   \n",
       "25                  SVM            150                0.8125   \n",
       "30                  SVM            175                0.6250   \n",
       "35                  SVM            180                0.7500   \n",
       "40                  SVM            200                0.5625   \n",
       "\n",
       "    Mean Cross-Validation Score  Score Difference  \n",
       "3                      0.730159          0.019841  \n",
       "8                      0.587302         -0.024802  \n",
       "13                     0.746032          0.003968  \n",
       "18                     0.682540          0.067460  \n",
       "23                     0.714286         -0.276786  \n",
       "28                     0.698413         -0.010913  \n",
       "33                     0.666667          0.020833  \n",
       "38                     0.730159         -0.042659  \n",
       "43                     0.650794          0.161706  \n",
       "2                      0.682540          0.067460  \n",
       "7                      0.682540          0.129960  \n",
       "12                     0.793651         -0.043651  \n",
       "17                     0.714286         -0.089286  \n",
       "22                     0.682540         -0.370040  \n",
       "27                     0.761905         -0.011905  \n",
       "32                     0.714286         -0.026786  \n",
       "37                     0.730159          0.144841  \n",
       "42                     0.777778         -0.152778  \n",
       "1                      0.507937         -0.007937  \n",
       "6                      0.539683          0.272817  \n",
       "11                     0.523810         -0.023810  \n",
       "16                     0.571429         -0.008929  \n",
       "21                     0.587302         -0.212302  \n",
       "26                     0.523810          0.101190  \n",
       "31                     0.492063          0.007937  \n",
       "36                     0.571429          0.053571  \n",
       "41                     0.507937         -0.007937  \n",
       "4                      0.777778         -0.027778  \n",
       "9                      0.761905         -0.011905  \n",
       "14                     0.730159          0.019841  \n",
       "19                     0.730159          0.019841  \n",
       "24                     0.746032          0.066468  \n",
       "29                     0.761905          0.113095  \n",
       "34                     0.730159          0.082341  \n",
       "39                     0.746032          0.128968  \n",
       "44                     0.746032          0.128968  \n",
       "0                      0.634921          0.115079  \n",
       "5                      0.650794          0.099206  \n",
       "10                     0.809524         -0.059524  \n",
       "15                     0.761905         -0.136905  \n",
       "20                     0.666667         -0.354167  \n",
       "25                     0.730159          0.082341  \n",
       "30                     0.730159         -0.105159  \n",
       "35                     0.714286          0.035714  \n",
       "40                     0.825397         -0.262897  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [10, 15, 25, 50, 100, 150, 175, 180, 200]\n",
    "\n",
    "# List of models to compare, including Random Forest\n",
    "models = {\n",
    "    'SVM': SVC(kernel='linear'),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'ANN': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000),\n",
    "    # Add Random Forest\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Variables to track the best accuracy and model\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_feature_count = None\n",
    "\n",
    "# Threshold for identifying potential overfitting\n",
    "overfitting_threshold = 0.05  # You can adjust this threshold as needed\n",
    "\n",
    "# Lists to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/Ashfa Fathima/OneDrive - University of Jaffna/Research/Data Sets/GSE160310/smallRNA/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Disese Group', axis=1)\n",
    "    y = df['Disese Group']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Standardize the features (important for some models)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model on the testing set\n",
    "        accuracy = model.score(X_test, y_test)\n",
    "\n",
    "        # Perform k-fold cross-validation (e.g., 5-fold cross-validation)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=3)\n",
    "\n",
    "        # Calculate the mean cross-validation score\n",
    "        mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "        # Calculate the difference between test accuracy and mean cross-validation score\n",
    "        score_difference = accuracy - mean_cv_score\n",
    "\n",
    "        # Check if the model may be overfitting based on the threshold\n",
    "        if score_difference <= 0.1:\n",
    "            # The model is not overfitting\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model_name\n",
    "                best_feature_count = count\n",
    "\n",
    "        # Append the results for the current model and feature count to the list\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Feature Count': count,\n",
    "            'Accuracy on Test Set': accuracy,\n",
    "            'Mean Cross-Validation Score': mean_cv_score,\n",
    "            'Score Difference': score_difference\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results DataFrame by model name\n",
    "results_df = results_df.sort_values(by=['Model', 'Feature Count'])\n",
    "\n",
    "# Print the best model and feature count\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Best Feature Count: {best_feature_count}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(\"\\nResults DataFrame:\")\n",
    "results_df\n",
    "\n",
    "# # Create a table plot of the results DataFrame\n",
    "# fig, ax = plt.subplots(figsize=(20, 12))\n",
    "# ax.axis('tight')\n",
    "# ax.axis('off')\n",
    "# ax.table(cellText=results_df.values, colLabels=results_df.columns,\n",
    "#          cellLoc='center', loc='center')\n",
    "\n",
    "# # Save the table plot as an image\n",
    "# table_image_path = 'results_table_fold3.png'\n",
    "# plt.savefig(table_image_path, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Print the path to the saved image\n",
    "# print(f\"Results table image saved at: {table_image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Result Table For 5 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 75 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 75 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 76 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 76 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 77 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 77 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 78 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 78 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 79 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 79 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 80 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 80 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 81 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 81 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 82 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 82 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 83 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 83 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 84 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 84 done for XGBoost\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count 85 done for Random Forest\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Feature count 85 done for XGBoost\n",
      "Best Model: XGBoost\n",
      "Best Feature Count: 76\n",
      "Best Accuracy: 0.875\n",
      "\n",
      "Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Best Hyperparameters</th>\n",
       "      <th>Accuracy on Test Set</th>\n",
       "      <th>Mean CV Score</th>\n",
       "      <th>Std CV Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>75</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.786895</td>\n",
       "      <td>0.035603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>76</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.825237</td>\n",
       "      <td>0.030173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>77</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.826804</td>\n",
       "      <td>0.027324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>78</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.806149</td>\n",
       "      <td>0.022058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>79</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.834117</td>\n",
       "      <td>0.028338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>80</td>\n",
       "      <td>{'bootstrap': True, 'max_depth': None, 'max_fe...</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.840741</td>\n",
       "      <td>0.014409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.785850</td>\n",
       "      <td>0.030833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>82</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.846201</td>\n",
       "      <td>0.019043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>83</td>\n",
       "      <td>{'bootstrap': False, 'max_depth': None, 'max_f...</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.821890</td>\n",
       "      <td>0.027393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>84</td>\n",
       "      <td>{'bootstrap': True, 'max_depth': None, 'max_fe...</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.826353</td>\n",
       "      <td>0.024064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>85</td>\n",
       "      <td>{'bootstrap': True, 'max_depth': None, 'max_fe...</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.783167</td>\n",
       "      <td>0.018703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>75</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.688513</td>\n",
       "      <td>0.052074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>76</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.740282</td>\n",
       "      <td>0.079435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>77</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 100, 'min_c...</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.730423</td>\n",
       "      <td>0.072709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>78</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.728615</td>\n",
       "      <td>0.084074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>79</td>\n",
       "      <td>{'subsample': 0.9, 'n_estimators': 100, 'min_c...</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.731718</td>\n",
       "      <td>0.064674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>80</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 100, 'min_c...</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.732795</td>\n",
       "      <td>0.087601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>81</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.704974</td>\n",
       "      <td>0.078868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>82</td>\n",
       "      <td>{'subsample': 0.9, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>0.085879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>83</td>\n",
       "      <td>{'subsample': 0.9, 'n_estimators': 100, 'min_c...</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.729756</td>\n",
       "      <td>0.079082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>84</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.719128</td>\n",
       "      <td>0.078216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>85</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 200, 'min_c...</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.678090</td>\n",
       "      <td>0.052602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Feature Count  \\\n",
       "0   Random Forest             75   \n",
       "2   Random Forest             76   \n",
       "4   Random Forest             77   \n",
       "6   Random Forest             78   \n",
       "8   Random Forest             79   \n",
       "10  Random Forest             80   \n",
       "12  Random Forest             81   \n",
       "14  Random Forest             82   \n",
       "16  Random Forest             83   \n",
       "18  Random Forest             84   \n",
       "20  Random Forest             85   \n",
       "1         XGBoost             75   \n",
       "3         XGBoost             76   \n",
       "5         XGBoost             77   \n",
       "7         XGBoost             78   \n",
       "9         XGBoost             79   \n",
       "11        XGBoost             80   \n",
       "13        XGBoost             81   \n",
       "15        XGBoost             82   \n",
       "17        XGBoost             83   \n",
       "19        XGBoost             84   \n",
       "21        XGBoost             85   \n",
       "\n",
       "                                 Best Hyperparameters  Accuracy on Test Set  \\\n",
       "0   {'bootstrap': False, 'max_depth': None, 'max_f...                0.8125   \n",
       "2   {'bootstrap': False, 'max_depth': None, 'max_f...                0.9375   \n",
       "4   {'bootstrap': False, 'max_depth': None, 'max_f...                0.8125   \n",
       "6   {'bootstrap': False, 'max_depth': None, 'max_f...                0.8750   \n",
       "8   {'bootstrap': False, 'max_depth': None, 'max_f...                0.8750   \n",
       "10  {'bootstrap': True, 'max_depth': None, 'max_fe...                0.8750   \n",
       "12  {'bootstrap': False, 'max_depth': None, 'max_f...                0.8750   \n",
       "14  {'bootstrap': False, 'max_depth': None, 'max_f...                0.8750   \n",
       "16  {'bootstrap': False, 'max_depth': None, 'max_f...                0.8125   \n",
       "18  {'bootstrap': True, 'max_depth': None, 'max_fe...                0.9375   \n",
       "20  {'bootstrap': True, 'max_depth': None, 'max_fe...                0.9375   \n",
       "1   {'subsample': 0.7, 'n_estimators': 200, 'min_c...                0.9375   \n",
       "3   {'subsample': 0.7, 'n_estimators': 200, 'min_c...                0.8750   \n",
       "5   {'subsample': 0.8, 'n_estimators': 100, 'min_c...                0.8125   \n",
       "7   {'subsample': 0.7, 'n_estimators': 200, 'min_c...                1.0000   \n",
       "9   {'subsample': 0.9, 'n_estimators': 100, 'min_c...                0.8750   \n",
       "11  {'subsample': 0.8, 'n_estimators': 100, 'min_c...                0.7500   \n",
       "13  {'subsample': 0.7, 'n_estimators': 200, 'min_c...                0.9375   \n",
       "15  {'subsample': 0.9, 'n_estimators': 200, 'min_c...                0.9375   \n",
       "17  {'subsample': 0.9, 'n_estimators': 100, 'min_c...                0.8125   \n",
       "19  {'subsample': 0.7, 'n_estimators': 200, 'min_c...                0.8125   \n",
       "21  {'subsample': 0.8, 'n_estimators': 200, 'min_c...                0.8125   \n",
       "\n",
       "    Mean CV Score  Std CV Score  \n",
       "0        0.786895      0.035603  \n",
       "2        0.825237      0.030173  \n",
       "4        0.826804      0.027324  \n",
       "6        0.806149      0.022058  \n",
       "8        0.834117      0.028338  \n",
       "10       0.840741      0.014409  \n",
       "12       0.785850      0.030833  \n",
       "14       0.846201      0.019043  \n",
       "16       0.821890      0.027393  \n",
       "18       0.826353      0.024064  \n",
       "20       0.783167      0.018703  \n",
       "1        0.688513      0.052074  \n",
       "3        0.740282      0.079435  \n",
       "5        0.730423      0.072709  \n",
       "7        0.728615      0.084074  \n",
       "9        0.731718      0.064674  \n",
       "11       0.732795      0.087601  \n",
       "13       0.704974      0.078868  \n",
       "15       0.747500      0.085879  \n",
       "17       0.729756      0.079082  \n",
       "19       0.719128      0.078216  \n",
       "21       0.678090      0.052602  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest\n",
    "\n",
    "# List of feature counts for datasets\n",
    "feature_counts = [75,76,77,78,79,80,81,82,83,84,85]\n",
    "\n",
    "# List of models to compare, including Random Forest\n",
    "models = {\n",
    "    # 'SVM': SVC(kernel='linear'),\n",
    "    # 'Naive Bayes': GaussianNB(),\n",
    "    # 'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    # 'ANN': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "}\n",
    "# xgbOOST\n",
    "# Variables to track the best accuracy and model\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_feature_count = None\n",
    "\n",
    "# Threshold for identifying potential overfitting\n",
    "overfitting_threshold = 0.05\n",
    "\n",
    "# Lists to store results\n",
    "results = []\n",
    "\n",
    "for count in feature_counts:\n",
    "    # Load the dataset\n",
    "    file_path = f\"C:/Users/ACER/OneDrive - University of Jaffna/UOJ/Education/Research/Data Sets/GSE160310/smallRNA/Feature Selection/Information Gain/data_k_{count}.csv\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    X = df.drop('Diagnosis', axis=1)\n",
    "    y = df['Diagnosis']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "    # Standardize the features (important for some models)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Define hyperparameter grids for GridSearchCV\n",
    "        param_grid_rf = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "\n",
    "        param_grid_xgb = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "            'max_depth': [3, 4, 5, 6],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'gamma': [0, 0.1, 0.2, 0.3]\n",
    "        }\n",
    "\n",
    "        if model_name == 'Random Forest':\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid_rf,\n",
    "                                       scoring='accuracy', cv=5, n_jobs=-1, verbose=2)\n",
    "        elif model_name == 'XGBoost':\n",
    "            grid_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid_xgb,\n",
    "                                             n_iter=100, scoring='accuracy', cv=5, n_jobs=-1, random_state=42, verbose=2)\n",
    "\n",
    "        # Fit the grid search to your data\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best hyperparameters and model\n",
    "        best_params = grid_search.best_params_\n",
    "        best_accuracy_cv = grid_search.best_score_\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Get the cross-validation scores\n",
    "        cross_val_scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "        # Store mean and standard deviation of cross-validation scores\n",
    "        mean_cv_score = np.mean(cross_val_scores)\n",
    "        std_cv_score = np.std(cross_val_scores)\n",
    "\n",
    "        # Evaluate the best model on the testing set\n",
    "        accuracy_test = best_model.score(X_test, y_test)\n",
    "\n",
    "        # Check if the model may be overfitting based on the threshold\n",
    "        if (accuracy_test - best_accuracy_cv) <= overfitting_threshold:\n",
    "            if accuracy_test > best_accuracy:\n",
    "                best_accuracy = accuracy_test\n",
    "                best_model_name = model_name\n",
    "                best_feature_count = count\n",
    "\n",
    "        # Append the results for the current model and feature count to the list\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Feature Count': count,\n",
    "            'Best Hyperparameters': best_params,\n",
    "            'Accuracy on Test Set': accuracy_test,\n",
    "            'Mean CV Score': mean_cv_score,\n",
    "            'Std CV Score': std_cv_score\n",
    "        })\n",
    "\n",
    "        # Print feature count done\n",
    "        print(f\"Feature count {count} done for {model_name}\")\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results DataFrame by model name\n",
    "results_df = results_df.sort_values(by=['Model', 'Feature Count'])\n",
    "\n",
    "# Print the best model and feature count\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Feature Count: {best_feature_count}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(\"\\nResults DataFrame:\")\n",
    "results_df\n",
    "\n",
    "# # Create a table plot of the results DataFrame\n",
    "# fig, ax = plt.subplots(figsize=(20, 12))\n",
    "# ax.axis('tight')\n",
    "# ax.axis('off')\n",
    "# ax.table(cellText=results_df.values, colLabels=results_df.columns,\n",
    "#          cellLoc='center', loc='center')\n",
    "\n",
    "# # Save the table plot as an image\n",
    "# table_image_path = 'results_table_fold5.png'\n",
    "# plt.savefig(table_image_path, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Print the path to the saved image\n",
    "# print(f\"Results table image saved at: {table_image_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
